# AutoTOS

Automatic Terms of Service and Privacy Policy parser and summarizer, powered by custom NLP techniques.

Created by [Andrew Mascillaro](https://github.com/intermezzio), [Spencer Ng](https://github.com/spencerng), [William Qin](https://github.com/MathTauAthogen), and [Eric Zheng](https://github.com/air-wreck). Winner of PennApps XXI's Best use of Google Cloud.

## Repository structure

This repository contains the following folders:

* `artifacts`: automatically downloaded files and those generated from data on the TOS;DR website. [See below](#data-files-and-artifacts) for a complete description.
* `config`: manually-created configuration files to facilitate TOS downloading, NLP model training, and deploying to the cloud
* `data`: scripts to download and clean data from TOS;DR into formats used for ML training
* `nlp`: scripts to test and train the custom NLP RoBERTa-based model
* `prediction`: scripts to serve the API backend and run evaluation on the trained model
* `docs`: code for the [AutoTOS website](http://autotos.me) frontend
* `gcp-docker`: archival scripts used to integrate AutoTOS with Google Cloud Platform via Docker


### Configuration files

* `cloudbuild.json`: configuration for Google Cloud Build for training the NLP model on the cloud. Unused.
* `cookies.config`: cookies used for the TOS;DR website login to parse TOS excerpts
* `mapped_classes.json`: manually-created mapping between original `classes.csv` descriptions of privacy topics and combined classes for AutoTOS’s model. Created generally based on original classes with high appearance frequency and/or similar descriptions.


## Data pipeline

1. Download `all.json` from source
2. `download_tos` package: use `all.json` to fetch both the full text of TOS’s and the excerpts that correspond to points. Produces labeled_excerpts.csv and classes.csv
  1. `__main__.py` 
  2. `fulltext.py`
  3. `point_text.py`
4. `cleanup.py`: uses `labeled_excerpts.csv`, adds extraneous (noise) data from the full TOS (broken down by sentence) for training, and generates `annotated_sentences.json`

![](https://gist.github.com/spencerng/63c95363c617bdbe0ec2c9e5d53785df/raw/d7c01cb18307a67d46df8391f3bdee362b3abe14/data-pipeline.svg)
" height="300"/>


### Data files and artifacts

* `all.json`: data from TOS;DR’s [GitHub repo](https://raw.githubusercontent.com/tosdr/tosdr.org/master/api/1/all.json) with the category of various “points” (annotated excerpts from TOS’s), without the corresponding excerpt text. However, it links to relevant parts of the TOS;DR website that do contain these excerpts
* `labeled_excerpts.csv`: list of excerpts from TOS texts. Each is labelled with the class ID and company slug.
* `classes.csv`: Mapping between class IDs and the descriptive title, score, and frequency of each class
* `tos/*.txt`: Full terms of service texts, as generated by `fulltext.py`
* `annotated_sentences.json`: sentences from TOS’s that either contain padding (sentences that belong to no class) or phrases labeled by class ID (as specified by classes.csv). Used directly by the training/testing model


## NLP pipeline

1. `split.py`: takes annotated_sentences.json and splits it into a 80/20 train/test set as `train_filter.json` and `test_filter.json`. Filters by the new classes `in mapped_classes.json`. All padding data (i.e. data without a class ID) are put into `train_filter.json`.
2. `train.py`: takes `train_filter.json` and builds a RoBERTa-based NLP model for sentence segmentation via `huggingface` or `finetune`. The `finetune`-based model is slightly more precise when testing with significantly fewer false positives in practice.
3. `test.py`: takes the generated model and `test_filter.json` and prints out model statistics

TensorFlow-based models are output to the `nlp/checkpoints` folder.

## API connection

* `predictor.py`
* `api.py`

To be written...